{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Dr. Long Chen (long.chen@glasgow.ac.uk) and Dr. Qunshan Zhao (Qunshan.Zhao@glasgow.ac.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The 10-Step Machine Learning Pipeline (My Version):\n",
    "1. Define business object/research questions\n",
    "2. Make sense of the data from a high level\n",
    "    - data types (number, text, object, etc.)\n",
    "    - continuous/discrete\n",
    "    - scales and distributions of different features\n",
    "3. Create the traning and test sets using proper sampling methods, e.g., random vs. stratified\n",
    "4. Correlation analysis (pair-wise and attribute combinations)\n",
    "5. Data cleaning (missing data, outliers, data errors)\n",
    "6. Data transformation via pipelines (categorical text to number using dummy variables, feature scaling via normalization/standardization, feature combinations)\n",
    "7. Train and cross validate different models and select the most promising one (Linear Regression, Lasso/Ridge/Elastic Net, and Random Forest were tried in this tutorial)\n",
    "8. Fine tune the model using trying different combinations of hyperparameters\n",
    "9. Evaluate the model with best estimators in the test set\n",
    "10. Launch, monitor, and refresh the model and system\n",
    "\n",
    "A few of general guideline:\n",
    "1. If you want to run a machine learning algorithm, you would better to have at least 2000-3000 samples, otherwise the performance is not necessary better than the traditional methods such as linear regression or GLM.\n",
    "2. Deep learning may have a better result if the sample size is very large (> 10,000 samples), otherwise general machine learning algorithm will be enough. If you want to learn more of the deep learning, you can check out this book: Dive into Deep Learning (https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2dbccbd6-138b-4f1b-9b23-fd60c7525c14",
    "_execution_state": "idle",
    "_uuid": "c9b1d5dff21d39260eb47af6fe7aac4bd03be233"
   },
   "outputs": [],
   "source": [
    "#import some necessary librairies\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a solution example for Kaggle challenge: House Prices: Advanced Regression Techniques. \n",
    "\n",
    "The dataset and problem explanation can be found here: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview. \n",
    "\n",
    "Question 1: How many features in the training dataset? How many featuers in the testing dataset? \n",
    "\n",
    "Question 2: How many records in the training and testing dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "59617b4b-d797-44ce-9142-05fbfd36aada",
    "_execution_state": "idle",
    "_uuid": "0e694d13459e3e200f6e2c6333c887cbad779ba9"
   },
   "outputs": [],
   "source": [
    "# Now let's import and put the train and test datasets in  pandas dataframe\n",
    " \n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3678529f-9d76-4853-88c5-4b2d230a85b6",
    "_execution_state": "idle",
    "_uuid": "3a32f51460a02fbe7a9122db55a740eb378dda97",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "##display the first five rows of the train dataset.\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ff37c1ba-8679-49e0-b3c8-9c53d01b1b04",
    "_execution_state": "idle",
    "_uuid": "816b1463b3dd0daf44949a1fa15ebfbc0e2f1235",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "##display the first five rows of the test dataset.\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#train.describe()#check the numbers of samples and features\n",
    "print(f\"The train data size before dropping Id feature is : {train.shape} \")\n",
    "print(f\"The test data size before dropping Id feature is : {test.shape} \")\n",
    "\n",
    "#Save the 'Id' column\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
    "train.drop(\"Id\", axis = 1, inplace = True)\n",
    "test.drop(\"Id\", axis = 1, inplace = True)\n",
    "\n",
    "#check again the data size after dropping the 'Id' variable\n",
    "print(f\"\\nThe train data size after dropping Id feature is : {train.shape} \")\n",
    "print(f\"The test data size after dropping Id feature is : {test.shape} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Sence of the Data\n",
    "What are the typical things we can learn from the basic statistics with visualizaitons? \n",
    "1. Do the data make sence? scan each column and see whether the data make sense at a high level. \n",
    "    \n",
    "    -  house value data is OK and this is our target variable, i.e., we want to build a model to predict this value.\n",
    " \n",
    "2. Feature scaling: you have noticed that the features have very different scales, which we need to handle later\n",
    "3. Distribution: from the histograms, we can tell many of them are skewed, i.e., having a long tail on one side or the other. In many cases, we need to transform the data so that they have more bell-shaped distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable\n",
    "\n",
    "\n",
    "**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first.\n",
    "\n",
    "Normality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(train['SalePrice'], fit=norm );\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "# #Get also the QQ-plot\n",
    "# fig = plt.figure()\n",
    "# res = stats.probplot(train['SalePrice'], plot=plt)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7d5829c4-b2f1-4ef3-8b02-11f02eb7aabf",
    "_execution_state": "idle",
    "_uuid": "228cb602f1c7a47d3c5250514cab57f7e7bc75e5"
   },
   "source": [
    "Target Variable: SalePrice\n",
    "\n",
    "Question: What did we do here and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "#Check the new distribution \n",
    "sns.distplot(train['SalePrice'] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['SalePrice'])\n",
    "\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "# #Get also the QQ-plot\n",
    "# fig = plt.figure()\n",
    "# res = stats.probplot(train['SalePrice'], plot=plt)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skew seems now corrected and the data appears more normally distributed. \n",
    "\n",
    "## Correlation Analysis\n",
    "\n",
    "Question: What's your findings in this big correlation matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corrmat = train.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#saleprice correlation matrix\n",
    "#number of variables for heatmap\n",
    "k = 10 \n",
    "# Question: what's the criterion to select these 10 variables and why? \n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = train[cols].corr()\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# scatterplot\n",
    "# Question: What's your findings in the scatterplot?\n",
    "# Hint: look at the relationships between basement and ground living area, year built vs year remodel. \n",
    "# sns.set()\n",
    "# cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt','YearRemodAdd']\n",
    "sns.pairplot(train[cols], height = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Data\n",
    "\n",
    "let's first  concatenate the train and test data in the same dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "y_train = train.SalePrice.values\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "#all_data = all_data[['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt','YearRemodAdd']]\n",
    "all_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "print(\"all_data size is : {}\".format(all_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=all_data_na.index, y=all_data_na)\n",
    "plt.xlabel('Features', fontsize=15)\n",
    "plt.ylabel('Percent of missing values', fontsize=15)\n",
    "plt.title('Percent missing data by feature', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#drop the missing data\n",
    "print(all_data.head())\n",
    "#all_data = all_data.drop(['Utilities'], axis=1)\n",
    "all_data = all_data.drop((missing_data[missing_data['Missing Ratio'] > 0.6]).index,1)\n",
    "all_data.isnull().sum().max() #just checking that there's no missing data missing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Check remaining missing values if any \n",
    "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_data[\"Utilities\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why we drop Utilities?\n",
    "all_data = all_data.drop(['Utilities'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_data.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Values\n",
    "\n",
    "You can fill N/A values by 0, mean, median, the nearby observations, most frequent categories.\n",
    "\n",
    "Sometimes you can delete the rows with many missing values, but we don't suggest to do it frequently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Check remaining missing values if any \n",
    "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSSubClass=The building class\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n",
    "\n",
    "\n",
    "# Changing OverallCond into a categorical variable\n",
    "all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n",
    "\n",
    "\n",
    "# Year and month sold are transformed into categorical features.\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "all_data['MoSold'] = all_data['MoSold'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparisons between LabelEncoder and OneHotEncoder: https://towardsdatascience.com/choosing-the-right-encoding-method-label-vs-onehot-encoder-a4434493149b\n",
    "\n",
    "LabelEncoder changes the categories to numbers. It is better to be used when the value of the categories is meaningful, such as the number of bedrooms. We will prefer to use LabelEncoder rather than OneHotEncoder if that is possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_data['LandSlope'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "cols = (   \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', \n",
    "         'Functional',   'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street',  'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold')\n",
    "# process columns, apply LabelEncoder to categorical features\n",
    "for c in cols:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[c].values)) \n",
    "    all_data[c] = lbl.transform(list(all_data[c].values))\n",
    "\n",
    "# shape        \n",
    "print('Shape all_data: {}'.format(all_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: what did we do here? Hint: this is a feature engineering process. \n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "465043f2-d687-4b1f-a6b4-1036859dfeb0",
    "_execution_state": "idle",
    "_uuid": "32b12bca723c5e867f7d7a7e179ff934a5fcdf30"
   },
   "source": [
    "## Outliers\n",
    "\n",
    "Let's explore these outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "637bd0fd-7508-41d1-b240-ea0e8598dddf",
    "_execution_state": "idle",
    "_uuid": "8903aa1a4a700aa2160edb3baf806f3800ae7d9a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "30304b82-5846-4142-bc31-b629158fb040",
    "_execution_state": "idle",
    "_uuid": "edf186dc5169e450392ee8f809cc3de5d10d7dbd"
   },
   "source": [
    "We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers.\n",
    "Therefore, we can safely delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6c5780b2-d4a8-42d9-b902-c6a23eef7d99",
    "_execution_state": "idle",
    "_uuid": "583bb417102d7bebb4aaf14bcb1aebcae86443bb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Deleting outliers\n",
    "train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n",
    "# train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']>700000)].index)\n",
    "#Check the graphic again\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(train['GrLivArea'], train['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e24be1ff-e186-4d0f-9ba1-64195c0eec4d",
    "_execution_state": "idle",
    "_uuid": "0f186c5806f14de1e9ea46ece78a4bed2a6830a7"
   },
   "source": [
    "### Note : \n",
    " Outliers removal is not always safe.  We decided to delete these two as they are very huge and  really  bad (extremely large areas for very low  prices). \n",
    "\n",
    "There are probably others outliers in the training data.   However, removing all them  may affect badly our models if ever there were also  outliers  in the test data. That's why, instead of removing them all, we will just manage to make some of our  models robust on them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39639caf-31a4-4401-a663-0ba9536b39bf",
    "_execution_state": "idle",
    "_uuid": "5a13a6e2a3e48975de9129d1593bd38df44a1069"
   },
   "source": [
    "**Getting dummy categorical features**\n",
    "\n",
    "Get dummy variables is the same as OneHot Encoder. \n",
    "\n",
    "See here for more information:\n",
    "https://towardsdatascience.com/one-hot-encoding-multicollinearity-and-the-dummy-variable-trap-b5840be3c41a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c8e63516-e4e2-4f36-a60e-1c8316392c60",
    "_execution_state": "idle",
    "_uuid": "acd44e283867425257ffd1fb2f4893cdbff43f67",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_data = pd.get_dummies(all_data)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "243cf047-c2ba-4ae5-a531-22ef9b7cfbfe",
    "_execution_state": "idle",
    "_uuid": "fe9d78c7e37142ee8089826eca3065e0fa5803c1"
   },
   "source": [
    "Getting the new train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0a75646f-1974-40ad-a085-ff7bc08454a5",
    "_execution_state": "idle",
    "_uuid": "89e464095544a53177d5a009b914ba4c660072a7"
   },
   "outputs": [],
   "source": [
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "461af83d-a928-4645-8512-5e4dbcaf7be0",
    "_execution_state": "idle",
    "_uuid": "10aab4cee97832560e2627a490e01e80c0ffb814"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "811925a6-341f-4cae-89c9-00983868a6b2",
    "_execution_state": "idle",
    "_uuid": "be4e4b315682b26359eba1ba3d65022aca9501e1"
   },
   "source": [
    "**Import librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "135e8ac5-ce46-4a5f-b205-13f827ef33b8",
    "_execution_state": "idle",
    "_uuid": "fc664fbe27561a3697d0210921107b0e14b7d211"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# xgboost installation guide: https://xgboost.readthedocs.io/en/latest/build.html#\n",
    "import xgboost as xgb\n",
    "# lightgbm installation guide: https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7aa6ebb9-27a7-4bd3-a7b2-4ddc6a0abbed",
    "_execution_state": "idle",
    "_uuid": "056b657c8f0de30d4708c600eabbb33684c64479"
   },
   "source": [
    "**Define a cross validation strategy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4a2b5181-44f2-4c74-b482-aae0f5afc25a",
    "_execution_state": "idle",
    "_uuid": "dc0d7a3013f349988b3f2c84a6c130d6ad350170"
   },
   "source": [
    "We use the **cross_val_score** function of Sklearn. However this function has not a shuffle attribut, we add then one line of code,  in order to shuffle the dataset  prior to cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f396260b-e182-4a87-9a2a-b92b9375ea6f",
    "_execution_state": "idle",
    "_uuid": "5c12551d092a6c5cf32d86398b054da7af3047b8"
   },
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the optimal parameters for each model with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'alpha':[10**-6, 10**-4, 10**-2, 0.1, 0.3, 0.5], 'l1_ratio': [0.1, 0.2,0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]} # try 6*10=60 combinations  \n",
    "]\n",
    "\n",
    "ENet = ElasticNet()\n",
    "grid_search = GridSearchCV(ENet, param_grid, cv=5, scoring='neg_mean_squared_error')  # each model is trained 5 times, so (12+6)*5 = 80 rounds of training in total\n",
    "grid_search.fit(train.values, y_train)\n",
    "grid_search.best_params_  # best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# GridSearchCV: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "# Tune the randome forest model\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators':[10, 30,50], 'max_features': [15,30,45,100,120,140,160,180,195]},  # try 3x3=9 combinations\n",
    "    #{'bootstrap': [False], 'n_estimators':[3, 10], 'max_features': [2, 3, 4]},  # try 2x3=6 combinations\n",
    "]\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error')  # each model is trained 5 times, so (12+6)*5 = 80 rounds of training in total\n",
    "grid_search.fit(train.values, y_train)\n",
    "grid_search.best_params_  # best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "grid_search.best_estimator_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "42e1565e-77a1-41a7-ac31-893e405d34ad",
    "_execution_state": "busy",
    "_uuid": "643ae2c4a88576ebbd55824ce8e654486087a6e0"
   },
   "source": [
    "## Base models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **L1/L2 Regularization**  : \n",
    "https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n",
    "\n",
    "L1 Regularization: Lasso regression\n",
    "\n",
    "L2 Regularization: Ridge regression\n",
    "\n",
    "L1+L2 Regularization: Elastic Net regression\n",
    "\n",
    "When you have many features, most of the model will have the overfitting problems. Regularization is a method to help select features in the model. \n",
    "\n",
    "The key difference between Lass and Ridge regression is that Lasso shrinks the less important featureâ€™s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "578f088d-1a84-41cb-b945-ec64800f2308",
    "_execution_state": "idle",
    "_uuid": "370125198a0cbbc9336cbf179f00a2ebb02cb063"
   },
   "source": [
    "-  **LASSO  Regression**  : \n",
    "\n",
    "This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's  **Robustscaler()**  method on pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "03f45cb7-0a40-45ea-94e8-64fd7ff1e8f6",
    "_execution_state": "idle",
    "_uuid": "2a50c954cb771d350c3092c3658486ba4d22aba5"
   },
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2c826f7b-ac66-421c-a7ae-29dfdd765bdb",
    "_execution_state": "idle",
    "_uuid": "30e9756cf63991715b48e8c53bc57906fc76f380"
   },
   "source": [
    "- **Elastic Net Regression** :\n",
    "\n",
    "again made robust to outliers\n",
    "\n",
    "Elastic Net regression: https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e635cc7e-caeb-4f8b-ae78-c41f8eb0be59",
    "_execution_state": "idle",
    "_uuid": "b614cf1bdee86a3b1cbdde05298f9f7ae023799b"
   },
   "outputs": [],
   "source": [
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.01, l1_ratio=.1, random_state=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7aae5316-4e32-4203-bff5-3b38c1f657c3",
    "_execution_state": "idle",
    "_uuid": "0775061bb477242f1332a048778e879ca540a216"
   },
   "source": [
    "- **Kernel Ridge Regression** :\n",
    "\n",
    "Kernel Ridge Regression: https://scikit-learn.org/stable/modules/kernel_ridge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "805343d9-0af6-43a2-a351-c0b25c62fcf0",
    "_execution_state": "idle",
    "_uuid": "3199c83513d93407c818ce1ed43c6c52e7f5a8c6"
   },
   "outputs": [],
   "source": [
    "KRR = make_pipeline(RobustScaler(),KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5a66c27c-be80-4ec0-8953-eaeb2a7dd2e7",
    "_execution_state": "idle",
    "_uuid": "14b60a7e4296cccb39042c9c625a1480d59a01c1"
   },
   "source": [
    "- **Advanced Tree-based Models **\n",
    "\n",
    "\n",
    "With **huber**  loss that makes it robust to outliers\n",
    "\n",
    "Gradient Boosting Regression: https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "af13332c-fd37-40bb-a078-6bad6caaa2ab",
    "_execution_state": "idle",
    "_uuid": "9a983f0f62a0dde7689b20a8e52022bb189478b4"
   },
   "outputs": [],
   "source": [
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d44ac87e-bf01-440b-ab22-b2868eb6ae48",
    "_execution_state": "idle",
    "_uuid": "53d7991f7dd03fcd7fb5ab1ec26fcd0614d002d3"
   },
   "source": [
    "- **XGBoost** :\n",
    "\n",
    "Explanation: https://xgboost.readthedocs.io/en/latest/tutorials/model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ed738a4c-c246-443c-a3c1-39df25f988b7",
    "_execution_state": "idle",
    "_uuid": "57c24b596ceb46d6f32ebf9501d672d7e469c15b"
   },
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a43ca74d-093c-4a56-a76c-b3223bf82fbc",
    "_execution_state": "idle",
    "_uuid": "460f3ccf7d5c33ea9f8a826bbf056d759e7b5119"
   },
   "source": [
    "- **LightGBM** :\n",
    "\n",
    "Explanation: https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dd84d7db-3f83-4e4e-b02f-7632ca5ee4ac",
    "_execution_state": "idle",
    "_uuid": "4c94cf90f0ef0d350c5e66f3bd397865bfcc61ae"
   },
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9e1eff6a-e937-45e4-96ef-41593e31e1bb",
    "_execution_state": "idle",
    "_uuid": "71bce529300e2f3d9f9f475d01bd7001258dbede"
   },
   "source": [
    "### Base models scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "84ddecce-7671-44e5-919d-97348bf413f4",
    "_execution_state": "idle",
    "_uuid": "cae4987b8ec89e90a90d7826c4ec98d315cac00b"
   },
   "source": [
    "Let's see how these base models perform on the data by evaluating the  cross-validation rmsle (root mean squared logarithmic error) error and error score standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2d0cc958-1654-425c-90ed-1ceb9edd7186",
    "_execution_state": "idle",
    "_uuid": "7d994349237b9304b0d17719e1af077e69288229",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "score = rmsle_cv(rf)\n",
    "print(\"\\nRandom Forest score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7cf6faaf-d69a-4268-b192-a9e60d207c28",
    "_execution_state": "idle",
    "_uuid": "b6d299b9d4a0cdb23ddd8459b3935da2948016d6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a1195106-2170-47f2-86a7-c4f3be683aa8",
    "_execution_state": "idle",
    "_uuid": "437dc093e88d661a369539520af1b4c37d1a0c1a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "score = rmsle_cv(KRR)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "43dd152f-7c49-41b6-8f8e-a5864b1e2a71",
    "_execution_state": "idle",
    "_uuid": "e9d8c4bd191f77d8d275f53c0c1a6cf344151294",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "score = rmsle_cv(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "30738ecc-39f8-44ed-9f42-68518beb7e6a",
    "_execution_state": "idle",
    "_uuid": "5f52ccf39d01165e61a7c6be8b788be4e58e286b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "41e0eab9-630d-48d3-905b-e4663aad2262",
    "_execution_state": "idle",
    "_uuid": "5cd5377ee097fbc6fd14b42b4ea654221b097e59",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "score = rmsle_cv(model_lgb)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1114bc71-7eb5-4a7c-97a1-42a69cc21130",
    "_execution_state": "idle",
    "_uuid": "06d3adc16585b54a85113882975297c67672ea07"
   },
   "source": [
    "## Stacking  models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56746043-0d77-4687-a8f2-ae494efae3a8",
    "_execution_state": "idle",
    "_uuid": "2410d2172ddc108475db49214c52c21e66aeee59"
   },
   "source": [
    "### Simplest Stacking approach : Averaging base models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "96d5979d-73ba-4810-bee2-e1a7a8de57f6",
    "_execution_state": "idle",
    "_uuid": "c6e3a67facbc786ddec2f56b40b4da37726d1be5"
   },
   "source": [
    "We begin with this simple approach of averaging base models.  We build a new **class**  to extend scikit-learn with our model and also to laverage encapsulation and code reuse ([inheritance][1]) \n",
    "\n",
    "\n",
    "  [1]: https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d0145496-896a-44e3-b01b-e12546328f06",
    "_execution_state": "idle",
    "_uuid": "5ecc887f1ab4001c872862cecf3a0b350ac51a23"
   },
   "source": [
    "**Averaged base models class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "49e44ad6-8dc4-4a67-8079-adbac934fec4",
    "_execution_state": "idle",
    "_uuid": "ff3ee5889bcac40847909c3a71285d2b8f9d431f"
   },
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "825eb99f-d509-4203-b0c6-4ff77f696322",
    "_execution_state": "idle",
    "_uuid": "f05bf966ea7a7b5e6f8ca5d641ebd11281d54d0d"
   },
   "source": [
    "**Averaged base models score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "18209a57-f46d-4ce7-8331-834f419c57f2",
    "_execution_state": "idle",
    "_uuid": "b66ef29c829b7122a2e8e2d187211039570973ac"
   },
   "source": [
    "We just average four models here **ENet, GBoost,  KRR and lasso**.  Of course we could easily add more models in the mix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d480916f-89e7-4bcc-9b9d-b54492591654",
    "_execution_state": "idle",
    "_uuid": "81ce9e148b7e735f465b4b6508511dea44fbf791",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "averaged_models = AveragingModels(models = (model_lgb, GBoost,  model_xgb))\n",
    "\n",
    "score = rmsle_cv(averaged_models)\n",
    "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "61f0f9af-9264-4945-829a-c629ed6a3299",
    "_execution_state": "idle",
    "_uuid": "0ca396a31059f16aff47e0d53d011865634e101e"
   },
   "source": [
    "We get again a better score by adding a meta learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1cc6527c-4705-4895-992f-0c3755b27cee",
    "_execution_state": "idle",
    "_uuid": "75e8303614ea910f93056a8bdc4cd9cfe62ecd46"
   },
   "source": [
    "## Ensembling StackedRegressor, XGBoost and LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "15f8fed4-bbf8-4eca-b400-8ea194010c78",
    "_execution_state": "idle",
    "_uuid": "5835af97aef41c60ea448988c606cd6a1f451712"
   },
   "source": [
    "We add **XGBoost and LightGBM** to the** StackedRegressor** defined previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5ab5b13e-78c1-49be-9bcb-e54a6bf119d7",
    "_execution_state": "idle",
    "_uuid": "9015eddf85323209a7729420affecb9940bdd7d3"
   },
   "source": [
    "We first define a rmsle evaluation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "232c3959-c6e1-4535-8ad4-62892edc3f06",
    "_execution_state": "idle",
    "_uuid": "07f9ef433905b61a08a36790254d6a34661f0653"
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "999a8cc6-5083-4fca-bc90-616ac2f3ef8b",
    "_execution_state": "idle",
    "_uuid": "b7b74b70e6514b7623bc67cfec2b4f5d37c98707"
   },
   "source": [
    "### Final Training and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "717b4b02-8bcf-4df3-8994-f6a113110115",
    "_execution_state": "idle",
    "_uuid": "115d9e90a84c33213f0f0de7d86b6098f29ca7d8"
   },
   "source": [
    "**StackedRegressor:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e64b2750-1e32-4e91-affb-e583d6ca8722",
    "_execution_state": "busy",
    "_uuid": "8936479533c4bb147ab09f1d2133d8bacbf9afc1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "averaged_models.fit(train.values, y_train)\n",
    "train_pred = averaged_models.predict(train.values)\n",
    "stacked_pred = np.expm1(averaged_models.predict(test.values))\n",
    "print(rmsle(y_train, train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6c322757-44c0-4c81-a319-1aa6ccdf440f",
    "_execution_state": "idle",
    "_uuid": "06a0eafc07a8dae002f3fc1499849ebf7ec014be"
   },
   "source": [
    "**XGBoost:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2af45055-47aa-4e26-84df-ba5726bdff54",
    "_execution_state": "idle",
    "_uuid": "c80de2558910e4091f087a99bfcb202f01033ad7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_xgb.fit(train, y_train)\n",
    "xgb_train_pred = model_xgb.predict(train)\n",
    "xgb_pred = np.expm1(model_xgb.predict(test))\n",
    "print(rmsle(y_train, xgb_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "22b2b135-2af8-4dbb-a8f0-1fcd7f745a66",
    "_execution_state": "idle",
    "_uuid": "b6d1cdcc2bfc08d0eb58135878008e6d64987089"
   },
   "source": [
    "**LightGBM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "995d4c8e-db72-4370-a1ec-50e0c761f09a",
    "_execution_state": "idle",
    "_uuid": "65398376dca67e2aa78576108a0bb8160031c111",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_lgb.fit(train, y_train)\n",
    "lgb_train_pred = model_lgb.predict(train)\n",
    "lgb_pred = np.expm1(model_lgb.predict(test.values))\n",
    "print(rmsle(y_train, lgb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "619452b2-c395-48fe-81ab-d6b1d355236b",
    "_execution_state": "idle",
    "_uuid": "07500cf506f6a90c6439c2dabf81ab966cf1c792",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''RMSE on the entire Train data when averaging'''\n",
    "\n",
    "print('RMSLE score on train data:')\n",
    "print(rmsle(y_train,stacked_train_pred*0.8 +\n",
    "               xgb_train_pred*0.1 + lgb_train_pred*0.1 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "844b5e21-7bd2-4a2b-9f7a-2e755ed06ecb",
    "_execution_state": "idle",
    "_uuid": "59443e95f66cb9e595cff9a3666824299239126b"
   },
   "source": [
    "**Ensemble prediction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3ec2c58f-6bee-46a6-a263-1fe2cf3569cb",
    "_execution_state": "idle",
    "_uuid": "18996472b775bd9114fea7f08c8a554d4dafe774"
   },
   "outputs": [],
   "source": [
    "ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "434ca649-2fa0-46a5-ab29-7f403448ddf7",
    "_execution_state": "idle",
    "_uuid": "c9f02561da543f4901dcd2051acbd6c197108dd5"
   },
   "source": [
    "**Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3db46af9-e18a-43bb-9699-45b851f835e5",
    "_execution_state": "idle",
    "_uuid": "93f6915cf25c7bb6b6fa6e74ad7b853387ac1db5"
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test_ID\n",
    "sub['SalePrice'] = ensemble\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
